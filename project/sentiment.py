# -*- coding: utf-8 -*-
"""Sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14ekbzUx0dcEkzMdwBAhbdf4fflz6lL4H
"""

# Install Kaggle library
!pip install -q kaggle
from google.colab import files
#upload the kaggle.json file you downloaded
uploaded = files.upload()
# make a diectoryin which kajggle.json is stored
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
#download the dataset into the colab(paste API command after !)
!kaggle datasets download -d snap/amazon-fine-food-reviews

!unzip amazon-fine-food-reviews.zip
import pickle
import pandas as pd
review = pd.read_csv("/content/Reviews.csv")
review.head()

review = review.drop(['ProductId','UserId','ProfileName','Id','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time','Summary'], axis=1)

print(review.info(memory_usage='deep'))

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(review['Text'], review['Positivity'], random_state = 0)

print('X_train first entry: \n\n', X_train[0])
print('\n\nX_train shape: ', X_train.shape)

vect = CountVectorizer().fit(X_train)
vect

feat = vect.get_feature_names()

X_train_vectorized = vect.transform(X_train)

"""TF IDF (term-frequency-inverse-document-frequency)."""

# ignore terms that appear in less than 5 documents
vect = TfidfVectorizer(min_df = 5).fit(X_train)
len(vect.get_feature_names())

# check the top 10 features for positive and negative
# reviews again, the AUC has improved
feature_names = np.array(vect.get_feature_names())
sorted_coef_index = model.coef_[0].argsort()

# print('Smallest Coef: \n{}\n'.format(feature_names[sorted_coef_index][:10]))
# print('Largest Coef: \n{}\n'.format(feature_names[sorted_coef_index][:-11:-1]))

feat = vect.get_feature_names()

cloud = WordCloud(width=1440, height=1080).generate(" ".join(feat))

X_train_vectorized = vect.transform(X_train)

"""**Bigrams**


Since our classifier misclassifies things like 'not good', we will use groups of words instead of single words. This method is called n grams (bigrams for 2 words and so on). Here we take 1 and 2 words into consideration.
"""

vect = CountVectorizer(min_df = 5, ngram_range = (1,2)).fit(X_train)
X_train_vectorized = vect.transform(X_train)
len(vect.get_feature_names())

feat = vect.get_feature_names()

# the number of features has increased again
# checking for the AUC
model = LogisticRegression()
model.fit(X_train_vectorized, y_train)

predictions = model.predict(vect.transform(X_test))

new_review = ['The food is not good, I would never buy them again']
print(model.predict(vect.transform(new_review)))

# there are still more misclassifications
# lets try with 3 grams
vect = CountVectorizer(min_df = 5, ngram_range = (1,3)).fit(X_train)
X_train_vectorized = vect.transform(X_train)
len(vect.get_feature_names())

feat = vect.get_feature_names()

model = LogisticRegression()
model.fit(X_train_vectorized, y_train)
pickle.dump(model,open(model.pkl,wb)),
pickle.dump(tfidf_model,open(vect.pkl,wb))

predictions = model.predict(vect.transform(X_test))
accuracy_score(y_test, predictions)

roc_auc = roc_auc_score(y_test, predictions)
print('AUC: ', roc_auc)
fpr, tpr, thresholds = roc_curve(y_test, predictions)

